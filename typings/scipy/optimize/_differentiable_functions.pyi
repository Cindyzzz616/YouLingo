"""
This type stub file was generated by pyright.
"""

FD_METHODS = ...
class _ScalarGradWrapper:
    """
    Wrapper class for gradient calculation
    """
    def __init__(self, grad, fun=..., args=..., finite_diff_options=...) -> None:
        ...
    
    def __call__(self, x, f0=..., **kwds):
        ...
    


class _ScalarHessWrapper:
    """
    Wrapper class for hess calculation via finite differences
    """
    def __init__(self, hess, x0=..., grad=..., args=..., finite_diff_options=...) -> None:
        ...
    
    def __call__(self, x, f0=..., **kwds): # -> csr_array | NDArray[Any]:
        ...
    


class ScalarFunction:
    """Scalar function and its derivatives.

    This class defines a scalar function F: R^n->R and methods for
    computing or approximating its first and second derivatives.

    Parameters
    ----------
    fun : callable
        evaluates the scalar function. Must be of the form ``fun(x, *args)``,
        where ``x`` is the argument in the form of a 1-D array and ``args`` is
        a tuple of any additional fixed parameters needed to completely specify
        the function. Should return a scalar.
    x0 : array-like
        Provides an initial set of variables for evaluating fun. Array of real
        elements of size (n,), where 'n' is the number of independent
        variables.
    args : tuple, optional
        Any additional fixed parameters needed to completely specify the scalar
        function.
    grad : {callable, '2-point', '3-point', 'cs'}
        Method for computing the gradient vector.
        If it is a callable, it should be a function that returns the gradient
        vector:

            ``grad(x, *args) -> array_like, shape (n,)``

        where ``x`` is an array with shape (n,) and ``args`` is a tuple with
        the fixed parameters.
        Alternatively, the keywords  {'2-point', '3-point', 'cs'} can be used
        to select a finite difference scheme for numerical estimation of the
        gradient with a relative step size. These finite difference schemes
        obey any specified `bounds`.
    hess : {callable, '2-point', '3-point', 'cs', HessianUpdateStrategy}
        Method for computing the Hessian matrix. If it is callable, it should
        return the  Hessian matrix:

            ``hess(x, *args) -> {LinearOperator, spmatrix, array}, (n, n)``

        where x is a (n,) ndarray and `args` is a tuple with the fixed
        parameters. Alternatively, the keywords {'2-point', '3-point', 'cs'}
        select a finite difference scheme for numerical estimation. Or, objects
        implementing `HessianUpdateStrategy` interface can be used to
        approximate the Hessian.
        Whenever the gradient is estimated via finite-differences, the Hessian
        cannot be estimated with options {'2-point', '3-point', 'cs'} and needs
        to be estimated using one of the quasi-Newton strategies.
    finite_diff_rel_step : None or array_like
        Relative step size to use. The absolute step size is computed as
        ``h = finite_diff_rel_step * sign(x0) * max(1, abs(x0))``, possibly
        adjusted to fit into the bounds. For ``method='3-point'`` the sign
        of `h` is ignored. If None then finite_diff_rel_step is selected
        automatically,
    finite_diff_bounds : tuple of array_like
        Lower and upper bounds on independent variables. Defaults to no bounds,
        (-np.inf, np.inf). Each bound must match the size of `x0` or be a
        scalar, in the latter case the bound will be the same for all
        variables. Use it to limit the range of function evaluation.
    epsilon : None or array_like, optional
        Absolute step size to use, possibly adjusted to fit into the bounds.
        For ``method='3-point'`` the sign of `epsilon` is ignored. By default
        relative steps are used, only if ``epsilon is not None`` are absolute
        steps used.
    workers : map-like callable, optional
        A map-like callable, such as `multiprocessing.Pool.map` for evaluating
        any numerical differentiation in parallel.
        This evaluation is carried out as ``workers(fun, iterable)``, or
        ``workers(grad, iterable)``, depending on what is being numerically
        differentiated.
        Alternatively, if `workers` is an int the task is subdivided into `workers`
        sections and the function evaluated in parallel
        (uses `multiprocessing.Pool <multiprocessing>`).
        Supply -1 to use all available CPU cores.
        It is recommended that a map-like be used instead of int, as repeated
        calls to `approx_derivative` will incur large overhead from setting up
        new processes.

        .. versionadded:: 1.16.0

    Notes
    -----
    This class implements a memoization logic. There are methods `fun`,
    `grad`, hess` and corresponding attributes `f`, `g` and `H`. The following
    things should be considered:

        1. Use only public methods `fun`, `grad` and `hess`.
        2. After one of the methods is called, the corresponding attribute
           will be set. However, a subsequent call with a different argument
           of *any* of the methods may overwrite the attribute.
    """
    def __init__(self, fun, x0, args, grad, hess, finite_diff_rel_step=..., finite_diff_bounds=..., epsilon=..., workers=...) -> None:
        ...
    
    @property
    def nfev(self): # -> int:
        ...
    
    @property
    def ngev(self): # -> int:
        ...
    
    @property
    def nhev(self): # -> int:
        ...
    
    def fun(self, x): # -> generic[Any] | bool | int | float | complex | str | bytes | memoryview[int]:
        ...
    
    def grad(self, x):
        ...
    
    def hess(self, x): # -> HessianUpdateStrategy | object | csr_array | NDArray[Any] | None:
        ...
    
    def fun_and_grad(self, x): # -> tuple[Any | generic[Any] | bool | int | float | complex | str | bytes | memoryview[int], Any]:
        ...
    


class _VectorFunWrapper:
    def __init__(self, fun) -> None:
        ...
    
    def __call__(self, x): # -> NDArray[Any]:
        ...
    


class _VectorJacWrapper:
    """
    Wrapper class for Jacobian calculation
    """
    def __init__(self, jac, fun=..., finite_diff_options=..., sparse_jacobian=...) -> None:
        ...
    
    def __call__(self, x, f0=..., **kwds): # -> csr_array | LinearOperator:
        ...
    


class _VectorHessWrapper:
    """
    Wrapper class for Jacobian calculation
    """
    def __init__(self, hess, jac=..., finite_diff_options=...) -> None:
        ...
    
    def __call__(self, x, v, J0=..., **kwds): # -> csr_array | LinearOperator | NDArray[Any] | None:
        ...
    
    def jac_dot_v(self, x, v):
        ...
    


class VectorFunction:
    """Vector function and its derivatives.

    This class defines a vector function F: R^n->R^m and methods for
    computing or approximating its first and second derivatives.

    Notes
    -----
    This class implements a memoization logic. There are methods `fun`,
    `jac`, hess` and corresponding attributes `f`, `J` and `H`. The following
    things should be considered:

        1. Use only public methods `fun`, `jac` and `hess`.
        2. After one of the methods is called, the corresponding attribute
           will be set. However, a subsequent call with a different argument
           of *any* of the methods may overwrite the attribute.
    """
    def __init__(self, fun, x0, jac, hess, finite_diff_rel_step=..., finite_diff_jac_sparsity=..., finite_diff_bounds=..., sparse_jacobian=..., workers=...) -> None:
        ...
    
    @property
    def nfev(self): # -> int:
        ...
    
    @property
    def njev(self): # -> int:
        ...
    
    @property
    def nhev(self): # -> int:
        ...
    
    def fun(self, x): # -> Array:
        ...
    
    def jac(self, x): # -> int_ | numpy.bool[builtins.bool] | float64 | ndarray[tuple[int], dtype[int_ | numpy.bool[builtins.bool] | float64]] | ndarray[tuple[int, int], dtype[int_ | numpy.bool[builtins.bool] | float64]] | ndarray[_Shape, dtype[int_ | numpy.bool[builtins.bool] | float64]] | csr_matrix | csr_array | <subclass of _csr_base* and sparray> | _csr_base | object | LinearOperator | _Array1D[float64] | _Array[tuple[int, int], float64] | NDArray[float64]:
        ...
    
    def hess(self, x, v): # -> ndarray[_Shape, dtype[int_ | numpy.bool[builtins.bool]]] | csr_array | LinearOperator | NDArray[Any] | HessianUpdateStrategy | None:
        ...
    


class LinearVectorFunction:
    """Linear vector function and its derivatives.

    Defines a linear function F = A x, where x is N-D vector and
    A is m-by-n matrix. The Jacobian is constant and equals to A. The Hessian
    is identically zero and it is returned as a csr matrix.
    """
    def __init__(self, A, x0, sparse_jacobian) -> None:
        ...
    
    def fun(self, x): # -> NDArray[Any] | _NotImplementedType | csr_array:
        ...
    
    def jac(self, x): # -> csr_array | NDArray[Any]:
        ...
    
    def hess(self, x, v): # -> csr_array:
        ...
    


class IdentityVectorFunction(LinearVectorFunction):
    """Identity vector function and its derivatives.

    The Jacobian is the identity matrix, returned as a dense array when
    `sparse_jacobian=False` and as a csr matrix otherwise. The Hessian is
    identically zero and it is returned as a csr matrix.
    """
    def __init__(self, x0, sparse_jacobian) -> None:
        ...
    


